{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDD는 두가지의 연산으로 이루어져 있다\n",
    "- Transformation\n",
    "- Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformation -> Lazy Execution 또는 Lazy Loading\n",
    "- 트랜스포메이션이 행해지면 RDD가 수행되는 것이 아니라, 새로운 RDD를 만들어 내고 \n",
    "- 그 새로운 RDD에 수행결과를 저장하게 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Action\n",
    "- 메서드로 이루어진 실행작업이며, 트랜스포메이션이 행해지고 나서 이루어지는 Evaluation작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-UVBAK5K6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sparkApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=sparkApp>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf().setMaster('local').setAppName('sparkApp')\n",
    "spark = SparkContext(conf=conf)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./data/test.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.textFile('./data/test.txt') # transformation\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = rdd.filter(lambda x :'spark' in x) # action\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDD 생성\n",
    "- 데이터를 직접 만든는 방법(parallelize), 외부 데이터를 로드 방법으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[5] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd = spark.parallelize(['test','this is a test rdd']) # rdd하나가 rom?에 저장\n",
    "sample_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'this is a test rdd']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd.collect() # 연산을 수행하는 함수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDD 자주 쓰는 연산 함수\n",
    "- collect() : RDD에 트랜스포메이션된 결과를 리턴하는 함수\n",
    "- map() : 연산을 수행하고 싶을때 사용하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[6] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = spark.parallelize(list(range(5)))\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=numbers.map(lambda x : x * x).collect()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- flatmap() : 리스트들의 원소를 하나의 리스트로 flatten해서 리턴하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'spark', 'hi', 'python', 'hi', 'sklearn', 'hi', 'django']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = spark.parallelize(['hi spark', 'hi python', 'hi sklearn','hi django'])\n",
    "unique_string = strings.flatMap(lambda x : x.split(' ')).collect()\n",
    "unique_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- filter() : 조건으로 필터링하는 연산자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[13] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = spark.parallelize(list(range(1,30,3)))\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 10, 16, 22, 28]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = num.filter(lambda x : x % 2 == 0).collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pair rdd란 key-value 쌍으로 이루어진 RDD\n",
    "- python tuple를 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[15] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pair RDD 생성\n",
    "pairRDD = spark.parallelize([(1,3),(1,5),(2,4),(3,3)])\n",
    "pairRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 8, 2: 4, 3: 3}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    i:j\n",
    "    for i, j in pairRDD.reduceByKey(lambda x,y : x+y).collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 25, 2: 16, 3: 9}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    i:j\n",
    "    for i, j in pairRDD.mapValues(lambda x : x**2).collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 3]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, <pyspark.resultiterable.ResultIterable at 0x278dd6045c0>),\n",
       " (2, <pyspark.resultiterable.ResultIterable at 0x278dd604630>),\n",
       " (3, <pyspark.resultiterable.ResultIterable at 0x278dd6046a0>)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 4, 3]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (1, 5), (2, 4), (3, 3)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 외부 데이터를 로드해서 RDD 생성하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./data/spark-rdd-name-customers.csv MapPartitionsRDD[32] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerRDD = spark.textFile('./data/spark-rdd-name-customers.csv')\n",
    "customerRDD # 불변이라 건들지를 못함 rdd라는 새로운 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alfreds Futterkiste,Germany'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Germany', 'Alfreds Futterkiste'),\n",
       " ('Mexico', 'Ana Trujillo Emparedados y helados'),\n",
       " ('Mexico', 'Antonio Moreno Taqueria'),\n",
       " ('UK', 'Around the Horn'),\n",
       " ('Sweden', 'Berglunds snabbkop'),\n",
       " ('Germany', 'Blauer See Delikatessen'),\n",
       " ('France', 'Blondel pere et fils'),\n",
       " ('Spain', 'Bolido Comidas preparadas'),\n",
       " ('France', \"Bon app'\"),\n",
       " ('Canada', 'Bottom-Dollar Marketse'),\n",
       " ('UK', \"B's Beverages\"),\n",
       " ('Argentina', 'Cactus Comidas para llevar'),\n",
       " ('Mexico', 'Centro comercial Moctezuma'),\n",
       " ('Switzerland', 'Chop-suey Chinese'),\n",
       " ('Brazil', 'Comercio Mineiro'),\n",
       " ('UK', 'Consolidated Holdings'),\n",
       " ('Germany', 'Drachenblut Delikatessend'),\n",
       " ('France', 'Du monde entier'),\n",
       " ('UK', 'Eastern Connection'),\n",
       " ('Austria', 'Ernst Handel'),\n",
       " ('Brazil', 'Familia Arquibaldo'),\n",
       " ('Spain', 'FISSA Fabrica Inter. Salchichas S.A.'),\n",
       " ('France', 'Folies gourmandes'),\n",
       " ('Sweden', 'Folk och fa HB'),\n",
       " ('Germany', 'Frankenversand'),\n",
       " ('France', 'France restauration'),\n",
       " ('Italy', 'Franchi S.p.A.'),\n",
       " ('Portugal', 'Furia Bacalhau e Frutos do Mar'),\n",
       " ('Spain', 'Galeria del gastronomo'),\n",
       " ('Spain', 'Godos Cocina Tipica'),\n",
       " ('Brazil', 'Gourmet Lanchonetes'),\n",
       " ('USA', 'Great Lakes Food Market'),\n",
       " ('Venezuela', 'GROSELLA-Restaurante'),\n",
       " ('Brazil', 'Hanari Carnes'),\n",
       " ('Venezuela', 'HILARIoN-Abastos'),\n",
       " ('USA', 'Hungry Coyote Import Store'),\n",
       " ('Ireland', 'Hungry Owl All-Night Grocers'),\n",
       " ('UK', 'Island Trading'),\n",
       " ('Germany', 'Koniglich Essen'),\n",
       " ('France', \"La corne d'abondance\"),\n",
       " ('France', \"La maison d'Asie\"),\n",
       " ('Canada', 'Laughing Bacchus Wine Cellars'),\n",
       " ('USA', 'Lazy K Kountry Store'),\n",
       " ('Germany', 'Lehmanns Marktstand'),\n",
       " ('USA', \"Let's Stop N Shop\"),\n",
       " ('Venezuela', 'LILA-Supermercado'),\n",
       " ('Venezuela', 'LINO-Delicateses'),\n",
       " ('USA', 'Lonesome Pine Restaurant'),\n",
       " ('Italy', 'Magazzini Alimentari Riuniti'),\n",
       " ('Belgium', 'Maison Dewey'),\n",
       " ('Canada', 'Mere Paillarde'),\n",
       " ('Germany', 'Morgenstern Gesundkost'),\n",
       " ('UK', 'North/South'),\n",
       " ('Argentina', 'Oceano Atlantico Ltda.'),\n",
       " ('USA', 'Old World Delicatessen'),\n",
       " ('Germany', 'Ottilies Kaseladen'),\n",
       " ('France', 'Paris specialites'),\n",
       " ('Mexico', 'Pericles Comidas clasicas'),\n",
       " ('Austria', 'Piccolo und mehr'),\n",
       " ('Portugal', 'Princesa Isabel Vinhoss'),\n",
       " ('Brazil', 'Que Delicia'),\n",
       " ('Brazil', 'Queen Cozinha'),\n",
       " ('Germany', 'QUICK-Stop'),\n",
       " ('Argentina', 'Rancho grande'),\n",
       " ('USA', 'Rattlesnake Canyon Grocery'),\n",
       " ('Italy', 'Reggiani Caseifici'),\n",
       " ('Brazil', 'Ricardo Adocicados'),\n",
       " ('Switzerland', 'Richter Supermarkt'),\n",
       " ('Spain', 'Romero y tomillo'),\n",
       " ('Norway', 'Sante Gourmet'),\n",
       " ('USA', 'Save-a-lot Markets'),\n",
       " ('UK', 'Seven Seas Imports'),\n",
       " ('Denmark', 'Simons bistro'),\n",
       " ('France', 'Specialites du monde'),\n",
       " ('USA', 'Split Rail Beer & Ale'),\n",
       " ('Belgium', 'Supremes delices'),\n",
       " ('USA', 'The Big Cheese'),\n",
       " ('USA', 'The Cracker Box'),\n",
       " ('Germany', 'Toms Spezialitaten'),\n",
       " ('Mexico', 'Tortuga Restaurante'),\n",
       " ('Brazil', 'Tradicao Hipermercados'),\n",
       " ('USA', \"Trail's Head Gourmet Provisioners\"),\n",
       " ('Denmark', 'Vaffeljernet'),\n",
       " ('France', 'Victuailles en stock'),\n",
       " ('France', 'Vins et alcools Chevalier'),\n",
       " ('Germany', 'Die Wandernde Kuh'),\n",
       " ('Finland', 'Wartian Herkku'),\n",
       " ('Brazil', 'Wellington Importadora'),\n",
       " ('USA', 'White Clover Markets'),\n",
       " ('Finland', 'Wilman Kala'),\n",
       " ('Poland', 'Wolski')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map연산자를 이용해서 콤마로 split하고 튜플로 리턴하는 구문을 작성해보자\n",
    "cusPairs = customerRDD.map(lambda x : (x.split(',')[1],x.split(',')[0]))\n",
    "cusPairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Germany', <pyspark.resultiterable.ResultIterable at 0x278dd652b00>),\n",
       " ('Mexico', <pyspark.resultiterable.ResultIterable at 0x278dd652ac8>),\n",
       " ('UK', <pyspark.resultiterable.ResultIterable at 0x278dd652c50>),\n",
       " ('Sweden', <pyspark.resultiterable.ResultIterable at 0x278dd652b70>),\n",
       " ('France', <pyspark.resultiterable.ResultIterable at 0x278dd652d30>),\n",
       " ('Spain', <pyspark.resultiterable.ResultIterable at 0x278dd652da0>),\n",
       " ('Canada', <pyspark.resultiterable.ResultIterable at 0x278dd652e10>),\n",
       " ('Argentina', <pyspark.resultiterable.ResultIterable at 0x278dd652e48>),\n",
       " ('Switzerland', <pyspark.resultiterable.ResultIterable at 0x278dd652e80>),\n",
       " ('Brazil', <pyspark.resultiterable.ResultIterable at 0x278dd652ef0>),\n",
       " ('Austria', <pyspark.resultiterable.ResultIterable at 0x278dd652f60>),\n",
       " ('Italy', <pyspark.resultiterable.ResultIterable at 0x278dd652fd0>),\n",
       " ('Portugal', <pyspark.resultiterable.ResultIterable at 0x278dd64f048>),\n",
       " ('USA', <pyspark.resultiterable.ResultIterable at 0x278dd64f0b8>),\n",
       " ('Venezuela', <pyspark.resultiterable.ResultIterable at 0x278dd64f0f0>),\n",
       " ('Ireland', <pyspark.resultiterable.ResultIterable at 0x278dd652cc0>),\n",
       " ('Belgium', <pyspark.resultiterable.ResultIterable at 0x278dd64f208>),\n",
       " ('Norway', <pyspark.resultiterable.ResultIterable at 0x278dd64f278>),\n",
       " ('Denmark', <pyspark.resultiterable.ResultIterable at 0x278dd64f2e8>),\n",
       " ('Finland', <pyspark.resultiterable.ResultIterable at 0x278dd64f358>),\n",
       " ('Poland', <pyspark.resultiterable.ResultIterable at 0x278dd64f3c8>)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupByKey() : 키값을 리스트 형태로 리턴하는 함수 - 키를 그룹잡아서 리턴 (중복도 없애줌)\n",
    "cusPairs.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Around the Horn',\n",
       " \"B's Beverages\",\n",
       " 'Consolidated Holdings',\n",
       " 'Eastern Connection',\n",
       " 'Island Trading',\n",
       " 'North/South',\n",
       " 'Seven Seas Imports']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uk에 사는 고객이름만 출력한다면? (dict 형식으로 만들어서 표현)\n",
    "groupKey = cusPairs.groupByKey().collect()\n",
    "\n",
    "# python 코드 \n",
    "# for country, name in groupkey :\n",
    "#     if country =='UK':\n",
    "#         for name in names :\n",
    "#             print(name)\n",
    "\n",
    "coustomerDict = {\n",
    "    country : [name for name in names] for country,names in groupKey\n",
    "    \n",
    "}\n",
    "\n",
    "coustomerDict['UK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Argentina',\n",
       " 'Argentina',\n",
       " 'Argentina',\n",
       " 'Austria',\n",
       " 'Austria',\n",
       " 'Belgium',\n",
       " 'Belgium',\n",
       " 'Brazil',\n",
       " 'Brazil',\n",
       " 'Brazil']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortByKey : Key오름차순으로 정렬을 해보고 상위 10개만 뽑는다면\n",
    "cusPairs.sortByKey().keys().collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Germany': 11,\n",
       " 'Mexico': 5,\n",
       " 'UK': 7,\n",
       " 'Sweden': 2,\n",
       " 'France': 11,\n",
       " 'Spain': 5,\n",
       " 'Canada': 3,\n",
       " 'Argentina': 3,\n",
       " 'Switzerland': 2,\n",
       " 'Brazil': 9,\n",
       " 'Austria': 2,\n",
       " 'Italy': 3,\n",
       " 'Portugal': 2,\n",
       " 'USA': 13,\n",
       " 'Venezuela': 4,\n",
       " 'Ireland': 1,\n",
       " 'Belgium': 2,\n",
       " 'Norway': 1,\n",
       " 'Denmark': 2,\n",
       " 'Finland': 2,\n",
       " 'Poland': 1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나라별 고객이 몇명인지를 카운트 해본다면?  # 딕셔너리 형식\n",
    "#cusPairs.collect()\n",
    "mapR = cusPairs.mapValues(lambda x : 1).reduceByKey(lambda x,y :x+y)\n",
    "# mapR\n",
    "\n",
    "{\n",
    "    i:j\n",
    "    for i, j in mapR.collect()\n",
    "}\n",
    "\n",
    "\n",
    "# mapR.collect() # 위 값과 동일하지만 이렇게 하면 튜플형식의 리스트로 가져옴 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spark dataframe은 RDD의 확장된 구조이다\n",
    "- 행, 열로 이루어진 내장 RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 생성\n",
    "- 스파크 세션을 이용한 생성\n",
    "- SQL 컨텍스트의 테이블을 통한 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x278dd66e080>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlCtx = SQLContext(spark)\n",
    "sqlCtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(brand='Ford', models={'name': 'Fiesta', 'price': '14260'}),\n",
       " Row(brand='Ford', models={'name': 'Focus', 'price': '18825'}),\n",
       " Row(brand='Ford', models={'name': 'Mustang', 'price': '26670'}),\n",
       " Row(brand='BMW', models={'name': '320', 'price': '40250'}),\n",
       " Row(brand='BMW', models={'name': 'X3', 'price': '41000'}),\n",
       " Row(brand='BMW', models={'name': 'X5', 'price': '60700'}),\n",
       " Row(brand='Fiat', models={'name': '500', 'price': '16495'})]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json 파일\n",
    "# json -> RDD -> DataFrame\n",
    "sample_json = spark.textFile('./data/cars.json')\n",
    "cars_df=sqlCtx.createDataFrame(sample_json.map(lambda x : json.loads(x)))\n",
    "cars_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- brand: string (nullable = true)\n",
      " |-- models: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_df.printSchema() # 테이블에 대한 요약정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|brand|              models|\n",
      "+-----+--------------------+\n",
      "| Ford|[name -> Fiesta, ...|\n",
      "| Ford|[name -> Focus, p...|\n",
      "| Ford|[name -> Mustang,...|\n",
      "|  BMW|[name -> 320, pri...|\n",
      "|  BMW|[name -> X3, pric...|\n",
      "|  BMW|[name -> X5, pric...|\n",
      "| Fiat|[name -> 500, pri...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_df.show() # 2차원의 행렬구조를 가지는 테이블 형식을 볼 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(brand='Ford', models={'name': 'Fiesta', 'price': '14260'})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_df.first() # 첫번째 row의 값들을 가져올 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|brand|\n",
      "+-----+\n",
      "| Ford|\n",
      "| Ford|\n",
      "| Ford|\n",
      "|  BMW|\n",
      "|  BMW|\n",
      "|  BMW|\n",
      "| Fiat|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 프레임에 대한 연산\n",
    "# select()\n",
    "\n",
    "cars_df.select('brand').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|price|\n",
      "+-----+\n",
      "|14260|\n",
      "|18825|\n",
      "|26670|\n",
      "|40250|\n",
      "|41000|\n",
      "|60700|\n",
      "|16495|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cars_df.select('models.price').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼의 타입 변환\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+\n",
      "|brand|   name|price|\n",
      "+-----+-------+-----+\n",
      "| Ford| Fiesta|14260|\n",
      "| Ford|  Focus|18825|\n",
      "| Ford|Mustang|26670|\n",
      "|  BMW|    320|40250|\n",
      "|  BMW|     X3|41000|\n",
      "|  BMW|     X5|60700|\n",
      "| Fiat|    500|16495|\n",
      "+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_price_type = cars_df.select('brand', 'models.name', 'models.price')\n",
    "cars_price_type = car_price_type.withColumn('price', car_price_type['price'].cast(IntegerType()))\n",
    "cars_price_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(brand='Ford', name='Fiesta', price=14260)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_price_type.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+\n",
      "|brand|   name|price|\n",
      "+-----+-------+-----+\n",
      "| Ford|Mustang|26670|\n",
      "|  BMW|    320|40250|\n",
      "|  BMW|     X3|41000|\n",
      "|  BMW|     X5|60700|\n",
      "+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 비교연산\n",
    "cars_price_type.filter(cars_price_type['price']>20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|brand|count|\n",
      "+-----+-----+\n",
      "|  BMW|    3|\n",
      "| Fiat|    1|\n",
      "| Ford|    3|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 그룹핑\n",
    "cars_price_type.groupBy('brand').count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
